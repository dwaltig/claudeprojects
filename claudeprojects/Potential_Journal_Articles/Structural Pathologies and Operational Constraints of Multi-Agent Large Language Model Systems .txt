Structural Pathologies and Operational Constraints of Multi-Agent Large Language Model Systems
The transition from monolithic Large Language Model (LLM) deployments to decentralized Multi-Agent Systems (MAS) represents a fundamental paradigm shift in artificial intelligence orchestration. This evolution is predicated on the hypothesis that task decomposition, role specialization, and iterative peer review can transcend the inherent reasoning limitations of individual models.[1, 2] However, empirical evidence suggests that this architectural complexity introduces a hidden complexity tax, manifesting as significant coordination overhead, context fragmentation, and non-linear error propagation.[3, 4] While MAS can achieve superior performance in specific parallelizable domains, such as financial reasoning or complex software engineering, these gains are often marginal and come at a prohibitive cost in terms of latency and token consumption.[5, 6] Systematic evaluations now reveal that the effectiveness of MAS is highly contingent on task properties, with many sequential reasoning tasks experiencing performance degradations of 39% to 70% compared to single-agent baselines.[7, 8]
The emergence of the social science of Large Language Models has begun to highlight that mind-like behavior is easy to over-interpret from text alone. Apparent cognition, morality, or bias claims within multi-agent societies should be treated with caution, as they are frequently driven by test leakage and superficial surface cues rather than stable inner traits.[9] When researchers remove hints or block shortcuts, the apparent skills of these agentic societies often vanish, suggesting that many multi-agent benefits may be measurement illusions rather than genuine emergent capabilities.[9] To strengthen the antifragility of systems built upon these models, it is necessary to document the specific documented failure modes and limitations that occur when these systems underperform compared to simpler, single-agent approaches.
Quantifying Coordination Overhead and Computational Cost
The primary impediment to the practical deployment of multi-agent systems is the non-linear growth of communication overhead. Unlike single-agent systems (SAS) where the reasoning process is contained within a single hidden state, MAS requires the externalization of reasoning into natural language tokens to facilitate inter-agent coordination.[3, 10] This process introduces a token multiplier effect that scales with both agent count and the complexity of the interaction protocol.
Token Consumption and Economic Multipliers
Research comparing SAS and MAS across diverse agentic tasks indicates that MAS consistently incurs significantly higher operational costs. Quantitative studies show that MAS can consume between 4 and 220 times more input tokens than SAS, while output token consumption typically increases by a factor of 2 to 12.[2, 11] This dramatic increase is attributed to the necessity of passing full or summarized conversation histories between agents to maintain a shared state. In many cases, Agent A’s output becomes Agent B’s input, leading to redundant processing where Agent B must read the context already generated by Agent A to make its own decision.[6, 12]
The real problem manifests when agents pass context between each other. If Agent A generates a 5,000-token response and Agent B must process all of it to reach a decision, the user pays for those 5,000 tokens twice. At scale, this context explosion becomes a primary driver of cost model failure.[12] Complexity often spirals before cost does, as tracing through multiple models and decision points makes debugging exponentially harder than original development.[12]
Architecture Metric
Single-Agent (SAS)
Multi-Agent (MAS)
Factor Increase
Input Token Load
1.0x (Baseline)
4.0x – 220.0x
4x to 220x
Output Token Load
1.0x (Baseline)
2.0x – 12.0x
2x to 12x
LLM Call Density
O(k)
O(n * k)
n-fold
Execution Latency
Low
High (Iterative)
Cumulative
The economic implications of this overhead are profound. In approximately 80% of test cases, MAS and SAS result in ties where both systems either succeed or fail together.[2] This suggests that for the vast majority of tasks, the sophisticated orchestration of MAS provides limited additional value while still incurring the latency and cost of multi-agent communication.[2]
Latency and Throughput Bottlenecks
The coordination overhead is not merely a financial concern but also a temporal one. Each inter-agent handoff represents a discrete API call, introducing cumulative network latency and inference time. In a sequential pipeline, the total latency is the sum of individual agent latencies plus the coordination overhead. In hierarchical or centralized architectures, the orchestrator becomes a throughput bottleneck. Under high load, orchestrator latency and CPU utilization can saturate, creating a ceiling on system-wide processing capacity.[10]
Execution latency requirements are often at odds with multi-agent designs. Real-time applications requiring sub-second response times struggle with architectures that require multiple coordination hops.[10] Furthermore, the concurrency explosion—where N users interact with M agents each performing P tool calls—can overwhelm traditional infrastructure.[4]
Coordination Strategy
Parallelization Factor
Communication Overhead
Latency Profile
Independent
n
1
Low
Decentralized
n
d * n
Moderate
Centralized
n
r * n
High
Hybrid
n
r * n + p * m
Very High
The interaction between efficiency and tool usage dominates performance metrics. Under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead.[7, 8] Total reasoning turns exhibit power-law growth with agent count, where interactions ∝n 
k
 , leading to prohibitive per-agent reasoning capacity as the team size exceeds a critical threshold.[13]
Optimal Agent Count Thresholds and the Inverted-U Relationship
A critical design challenge in MAS is determining the optimal number of agents. While some early theories suggested that more agents would lead to monotonic performance improvements, empirical evidence identifies a clear Inverted-U relationship between agent count and system effectiveness.[14]
Evidence for Optimal Team Sizes
Systematic reviews across high-stakes domains, such as clinical medicine, show that MAS effectiveness typically peaks at 4 to 5 agents.[14] Beyond this threshold, performance begins to decline due to increased coordination complexity, logical contradictions, and the propagation of noise.[14] This trend is statistically represented by a negative regression coefficient (β=−8.815), indicating that adding more agents without structural adjustments actively degrades system accuracy.[14]
Team Size
Performance Dynamic
Observed Primary Cause
1 Agent
Baseline
Model capability limits
2-3 Agents
Improvement
Task decomposition benefits
4-5 Agents
Optimal Peak
Maximum specialization synergy
6-9 Agents
Degradation
Coordination overhead > Benefits
10+ Agents
Saturation/Collapse
Consensus noise and context loss
Scaling patterns also diverge based on the underlying model family. For instance, Gemini-2.0 Flash exhibits a clear optimum at 7 agents before degradation, while the more capable Gemini-2.5 Pro peaks earlier, likely because its higher single-agent baseline leaves less room for collaborative gains.[15] This indicates that the optimal number of agents is not an absolute constant but depends on the interaction between model capacity and coordination strategy.[15]
Capability Saturation and Task Type Influence
A phenomenon known as capability saturation occurs when coordination yields diminishing or negative returns (β=−0.408,p<0.001) once single-agent baselines exceed approximately 45%.[8, 15] When a model is already sufficient for a task, the extra agent orchestration is not required and may introduce new failure modes.[16]
The most striking evidence of MAS failure is found in sequential reasoning tasks. In benchmarks like PlanCraft, which requires spatiotemporal planning under constraints, every multi-agent variant tested degraded performance by 39% to 70%.[8, 15] This failure is primarily because coordination overhead fragments reasoning capacity under fixed computational budgets, and the strictly sequential nature of the tasks cannot benefit from the parallel processing strengths of MAS.[13, 15] In contrast, parallelizable tasks like financial reasoning show significant gains (+80.9%) from centralized coordination.[15]
Task Domain
MAS vs SAS Performance
Optimal Strategy
Financial Reasoning
+80.9%
Centralized
Web Navigation
+9.2%
Decentralized
Sequential Planning
-39% to -70%
Single-Agent
Scripting/Coding
-4.4% to -35.3%
Single-Agent
Context Fragmentation and Knowledge Loss Mechanisms
Context fragmentation is an architectural limitation that occurs when tasks are decomposed across isolated agents executing within their own limited context windows.[4] This distribution of labor creates execution boundaries that frequently lead to knowledge loss and cascading misinterpretations.
Isolated Execution and Information Dissipation
When a complex task is split across agents, they often make locally reasonable decisions based on incomplete information. In clinical settings, an agent responsible for medication checks might approve a nephrotoxic drug because information about a patient's elevated creatinine—flagged by a different labs-review agent—is not present in its specific context window.[4] This isolation results in implicit decision conflicts where Agent A assumes an aggressive intervention strategy while Agent B assumes a conservative one, leading to subtle contradictions that are difficult for human reviewers to catch.[4]
Agents often operate with disconnected models of the world. Information gathered by one agent may not be shared effectively, leading to repeated work or critical knowledge gaps.[17] As multi-agent conversations extend, the probability of critical information being excluded from the accessible context increases dramatically. This information loss undermines the primary advantage of multi-agent systems: the integration of diverse, specialized perspectives.[18]
Mechanism
Description
Impact on System
Summarization Attrition
Compressing history for handoffs.
Loss of nuance and vital details.
Context Sliding
Tokens falling out of the window.
Loss of original user constraints.
Memory Inconsistency
Heterogeneous local views.
Contradictory agent decisions.
Orchestration Gaps
Failed detail communication.
Duplicated work or missed steps.
The Context Crisis in Production
Managing multiple context files or shared memory stores often leads to context drift. Systems without a dedicated context broker rely on static summaries, which are prone to information decay.[19] When inputs consume more than 50% of a model's capacity, the lost-in-the-middle phenomenon peaks, where the model ignores central instructions in favor of more recent prompts.[20] In MAS, the constant exchange of coordination messages fills the context window with meta-dialogue, effectively pushing the actual task data into the middle or out of the window entirely.[18, 20]
Large windows enable reasoning across entire documents, but they do not eliminate reasoning errors. Rather, they shift where errors manifest. Forcing segmentation through multi-agent task splitting often increases engineering overhead and degrades coherence compared to processing the entire task in one long-context prompt.[20]
Error Propagation and Cascading Hallucinations
Multi-agent systems do not just accumulate single-agent risks; they transform and amplify them. A collection of safe agents does not guarantee a safe collection of agents, as interactions create emergent failure modes such as cascading reliability failures and conformity bias.[21, 22]
Mechanisms of Systemic Misinformation
LLMs are prone to hallucinations, but agentic systems can amplify these through self-reflection, memory reinforcement, or multi-agent interaction.[23] A hallucination from a single agent (e.g., a fabricated patient ID or a non-existent research citation) can be consumed by downstream agents as ground truth.[4] This propagation makes testing complexity explode, as every possible interaction pattern must be validated to prevent error reinforcement.[4]
Failure Level
Mechanism
Result
Node-Level
Critical agent failure.
Bottlenecks entire system.
Edge-Level
Downstream overthinking.
Reasoning degradation from noise.
Path-Level
Propagation of indecision.
Irreversible final errors.
Independent agents have been found to amplify errors 17.2-fold through unchecked propagation, whereas centralized coordination, which incorporates verification bottlenecks, contains this to a 4.4-fold amplification.[15] Despite these safeguards, the inherent probabilistic nature of LLMs means that accumulated errors across agents result in suboptimal workflows and excessive computational costs.[24]
Conformity Bias and Groupthink
Multi-agent systems are susceptible to herding behavior, where agents abandon correct judgments in favor of an incorrect majority opinion.[25, 26] This conformity bias is influenced by several factors, including majority size and interaction time. Experiments using the BenchForm benchmark reveal that agents are more likely to conform when exposed to confident but incorrect tones from their peers.[27]
This phenomenon creates a dangerous false consensus. In governed environments, such as public sector decision-making, conformity bias among agents overseeing infrastructure risk evaluations could lead to unresolved vulnerabilities.[28, 29] Furthermore, monoculture collapse occurs when agents built on similar base models exhibit correlated vulnerabilities, failing simultaneously on the same edge cases.[21, 22]
Bias Type
Driver
Consequence
Conformity Bias
Majority pressure/Confident tone.
Independent evaluation failure.
Monoculture Collapse
Shared model vulnerabilities.
System-wide resilience failure.
Herding Behavior
Majority size/Question difficulty.
Abandonment of correct outliers.
Deficient Theory of Mind
Misaligned assumptions of peers.
Coordination breakdown.
The MAST Taxonomy: A Comprehensive Analysis of Failure Modes
To systematically diagnose why multi-agent systems fail, the Multi-Agent System Failure Taxonomy (MAST) identifies 14 unique failure modes clustered into three primary categories: Specification issues, Inter-agent misalignment, and Task verification failures.[30, 31]
Category 1: Specification and System Design Failures
Failures in this category often stem from the structural configuration of the MAS rather than the limitations of individual agents. Good MAS design requires an organizational understanding, as even organizations of sophisticated individuals can fail if the structure is flawed.[32]
Failure Mode
Prevalence
Description
1.1 Disobey Task Specification
13.4% - 17.1%
Failing to achieve the intended task objectives.
1.2 Disobey Role Specification
15.0%
Agents acting outside their defined persona/logic.
1.3 Step Repetition
13.2% - 18.6%
Redundant execution of subtasks.
1.4 Loss of Conversation History
6.8% - 12.4%
Fragmentation leads to context drift.
1.5 Unaware of Termination
1.0%
Continuing execution after goals are met.
The prevalence of step repetition and disobedience suggests that the coordination protocols used in current MAS are often brittle. Agents may lose track of their role in the middle of a long conversation, leading to catastrophic derailment where the conversation resets or the system fails to ask for necessary clarification.[30, 33]
Category 2: Inter-Agent Misalignment
Misalignment occurs when agents fail to synchronize their state or intentions, leading to information withholding or reasoning-action mismatches.[30, 31]
Failure Mode
Prevalence
Description
2.1 Conversation Reset
1.9%
Progress lost due to unexpected state resets.
2.2 Fail to Ask for Clarification
2.8%
Proceeding with ambiguous data.
2.3 Task Derailment
12.4%
Losing focus on the primary user goal.
2.4 Information Withholding
8.2% - 9.6%
Critical context not shared between peers.
2.5 Ignored Agent Input
6.2% - 7.8%
Disregarding peer contributions.
2.6 Reasoning-Action Mismatch
0.5% - 1.5%
Actions taken contradict the agent's logic.
These findings indicate that identified failures often require more complex solutions than simple prompt engineering. Achieving robust MAS reliability points toward the need for fundamental redesigns that move beyond isolated fixes.[30]
Category 3: Task Verification and Termination
Verification failures represent cases where the system provides an incorrect final output while claiming it is correct, or terminates the process prematurely.[30, 31]
Failure Mode
Prevalence
Description
3.1 Premature Termination
7.1% - 9.1%
Stopping before the subtask is finished.
3.2 No/Incomplete Verification
1.5% - 1.9%
Skipping quality control checks.
3.3 Incorrect Verification
2.8% - 3.3%
False positive verification of errors.
Correctness in state-of-the-art MAS like ChatDev can be as low as 25% due to these verification errors.[32] While better specifications can mitigate some issues, they are insufficient for the full taxonomy of failures, suggesting that model improvements alone will not fix poorly structured organizations.[33]
Economic Analysis and Cost-Effectiveness Realities
As LLM capabilities improve, the traditional advantages of multi-agent systems are diminishing. Frontier models like GPT-4o or Gemini-2.0 Flash are increasingly capable of handling complex reasoning independently, making the high cost of MAS orchestration harder to justify.[2]
Diminishing Returns of Specialized Collaboration
The performance gap between MAS and SAS is narrowing. In code generation tasks using MetaGPT on HumanEval, the MAS improvement over SAS dropped from 10.7% with earlier models to just 3.0% with Gemini-2.0 Flash.[2] This minor gain comes at the cost of consuming 4 to 220 times more input tokens, making it significantly less efficient for practical applications.[2, 6]
Workflow Component
SAS Cost
MAS Cost
Delta
Inference Tokens
Base
4x - 220x
Prohibitive
Development Time
Moderate
High
Substantial
Maintenance/Audit
Linear
Exponential
Severe
Tool Compute
Base
N * Base
High
MAS introduces practical challenges including increased complexity and higher computational costs. For tasks that are already well-suited for a single agent, the benefits of collaboration are limited, and improvements are often linked to the tools implemented rather than the agents' coordination efforts.[14]
ROI and Hybrid Deployment Paradigms
To manage these costs, hybrid paradigms like Agent Routing and Agent Cascade have emerged. Agent Routing uses an LLM-based rater to assess task complexity and route requests to either a single agent or a multi-agent system based on need. This achieves up to 2% better accuracy at only 50% of the cost of a full MAS baseline.[2]
Paradigm
Accuracy Gain
Cost Reduction
Success Rate
Agent Routing
+2.0%
50.0%
High
Agent Cascade
+1.1% - 12.0%
20.0% - 88.1%
Very High
Hybrid Agent
88.8% GCR
N/A
Optimal ROI
The Hybrid Agent architecture, which combines strengths of ReAct and CoT, has been highlighted as consistently high-performing with the highest Return on Investment (ROI).[34, 35] Rather than uniform enhancement, developers can use confidence-guided tracing to identify critical agents that bottleneck performance and selectively upgrade only those nodes, minimizing costs while improving system accuracy.[2, 6]
Structural Paths to Robustness: Transactional Safeguards and Memory
The fundamental limitations of MAS—unreliable self-validation, context loss, and lack of transactional safeguards—require architectural interventions beyond simple prompting.
Transactional Logic and Failure Recovery
Frameworks like SagaLLM extend the Saga pattern to multi-agent workflows. This ensures workflow-wide consistency and recovery through modular checkpointing and compensable execution.[36] Traditional MAS often fail to ensure coordinated success; for example, in a travel booking scenario, the system might book a hotel but fail to book a flight, leaving the user with an active reservation for a trip they cannot take.[36]
Transactional Feature
Function
Failure Prevention
Selective Retention
Filters critical information for memory.
Prevents context loss.
Dependency Tracking
Maintains prerequisites for rollback.
Prevents inconsistent states.
Compensatory Logic
Reverses successful subtasks on failure.
Ensures transactional integrity.
Structured Storage
Organizes specifications and justifications.
Enhances auditability.
SagaLLM addresses the foundational limits of LLM self-verification by incorporating temporal-spatial context tracking and external verification mechanisms to validate inter-agent dependencies.[36]
Heterogeneous Memory and Role Adherence
The limitations of fixed-size context windows are exacerbated in MAS where the volume of information exchanged grows with the number of agents. Intrinsic Memory Agents solve this by providing heterogeneous memory for each agent, allowing them to maintain specialized roles without being overwhelmed by the global conversation history.[18, 37]
This structured approach reflects both historical context and new information while maintaining the benefits of specialized expertise. Periodic condensation of history into concise summaries allows for long-term memory maintenance, reducing the probability of critical information loss as tasks extend over time.[18, 37]
Memory Type
Architecture
Benefit
Short-Term
Context Window (Self-Attention)
High immediate reasoning.
Long-Term
Summarization/RAG
Sustained problem solving.
Intrinsic
Agent-Specific Heterogeneous
Preserves specialized perspective.
Shared
Centralized Memory Buffer
Facilitates coordination.
Ultimately, the goal of multi-agent engineering is to move from heuristic-based designs to a principled approach to scaling. By understanding the tool-coordination trade-offs and capability saturation points, practitioners can design systems that avoid the structural pathologies common in first-generation MAS.[8, 13]
Structural Analysis of Peer-to-Peer vs. Hierarchical Architectures
The effectiveness of a multi-agent system is heavily influenced by its communication topology. Research identifies four primary coordination architectures: collaborative (peer-to-peer), sequential (pipeline), competitive (selection-based), and hierarchical (manager-worker).[5]
Topographical Performance Trade-offs
Centralized coordination significantly improves performance on parallelizable tasks, such as financial reasoning, by roughly 80.9%. However, decentralized coordination excels in dynamic environments like web navigation, providing a modest gain where centralized structures might be too rigid.[7, 15]
Architecture
Parallelizable Task Gain
Dynamic Task Gain
Sequential Task Loss
Centralized
+80.9%
Minimal
-39% to -70%
Decentralized
Minimal
+9.2%
-39% to -70%
Independent
+57.0%
Catastrophic (-35%)
-39% to -70%
Independent architectures suffer from 17.2x error amplification because they lack a validation bottleneck where an orchestrator reviews outputs.[13, 15] Furthermore, as task complexity increases, the coordination overhead grows non-linearly, spend more time on orchestration than meaningful work.[10] Systems processing hundreds of concurrent requests require architectures that minimize coordination hops and enable parallel paths to meet latency requirements.[10]
Governance and Risk Analysis in Critical Infrastructures
As deployments evolve from single agents to multi-agent networks, the risk landscape transforms. A governed environment requires control over the configuration and deployment of all agents.[21, 22] The multi-agent paradigm challenges traditional governance through emergent behaviors and the amplification of existing risks. Cognitive limitations in individual agents become cascading hallucinations in the network, and individual biases become collective blind spots.[21, 22]
Governance frameworks must adjust to address systemic risks. Interactions between agents can compromise the dependability of services managing sensitive information, such as health records or taxation data.[28, 29] Pre-deployment testing is insufficient; active systems require ongoing monitoring to detect conversational loops or information loss that derails task completion.[21, 28] Organizations should align model diversity and verification protocols with overarching policy objectives to prevent operational inefficiencies.[28, 29]
Risk Factor
Single-Agent Impact
Multi-Agent Impact
Hallucination
Isolated error.
Cascading systemic misinformation.
Communication
Internal prompt failure.
Inter-agent loop/Derailment.
Bias
Latent model behavior.
Conformity-reinforced blind spots.
Scale
Linear resource growth.
Non-linear concurrency explosion.
Ultimately, the analysis of documented failure modes underscores that multi-agent systems are a coordination layer atop LLMs. The winners in the AI stack will not be those who merely expose agents, but those who master agent collaboration—task decomposition, shared context, and conflict resolution—while managing the inherent costs and reliability risks.[3] Transitioning from solitary models to artificial communities requires an engineered communication topology and a robust memory substrate to sustain long-horizon projects.[38]
--------------------------------------------------------------------------------
1. Multi-Agent Collaboration Mechanisms: A Survey of LLMs - arXiv, https://arxiv.org/html/2501.06322v1
2. Single-agent or Multi-agent Systems? Why Not Both? | alphaXiv, https://www.alphaxiv.org/overview/2505.18286v1
3. Understanding Multi‑Agent Systems: Coordination, Commoditization, and the AI Stack, https://sider.ai/blog/ai-tools/understanding-multi-agent-systems-coordination-commoditization-and-the-ai-stack
4. The hidden complexity tax: The problems with generalist multi-agent ..., https://www.corti.ai/stories/the-problems-with-generalist-multi-agent-frameworks
5. Multi-Agent Coordination Strategies vs. Retrieval-Augmented Generation in LLMs: A Comparative Evaluation - MDPI, https://www.mdpi.com/2079-9292/14/24/4883
6. Single-agent or Multi-agent Systems? Why Not Both? - arXiv, https://arxiv.org/html/2505.18286v1
7. Towards a Science of Scaling Agent Systems - arXiv, https://arxiv.org/html/2512.08296v1
8. [2512.08296] Towards a Science of Scaling Agent Systems - arXiv, https://arxiv.org/abs/2512.08296
9. The Emergence of Social Science of Large Language Models (a systematic review of 270 studies, 27 Oct 2025) : r/singularity - Reddit, https://www.reddit.com/r/singularity/comments/1prozjr/the_emergence_of_social_science_of_large_language/
10. Building Production-Ready Multi-Agent Systems: Architecture Patterns and Operational Best Practices, https://www.getmaxim.ai/articles/best-practices-for-building-production-ready-multi-agent-systems/
11. Single-agent or Multi-agent Systems? Why Not Both? - ChatPaper, https://chatpaper.com/paper/141388
12. Orchestrating multiple AI agents together—when does the cost and complexity actually spiral? - Enterprise Adoption & Procurement, https://community.latenode.com/t/orchestrating-multiple-ai-agents-together-when-does-the-cost-and-complexity-actually-spiral/57990
13. [論文評述] Towards a Science of Scaling Agent Systems - Moonlight, https://www.themoonlight.io/tw/review/towards-a-science-of-scaling-agent-systems
14. AI Agents in Clinical Medicine: A Systematic Review - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12407621/
15. (PDF) Towards a Science of Scaling Agent Systems - ResearchGate, https://www.researchgate.net/publication/398513043_Towards_a_Science_of_Scaling_Agent_Systems
16. Evaluating the Sufficiency of Single-Agent LLM Systems for Algorithmic Problem Solving in Support and Operations - BTH - DiVA portal, http://bth.diva-portal.org/smash/record.jsf?pid=diva2:2018511
17. Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications | by Eleventh Hour Enthusiast | Medium, https://medium.com/@EleventhHourEnthusiast/advancing-multi-agent-systems-through-model-context-protocol-architecture-implementation-and-5146564bc1ff
18. HETEROGENEOUS MULTI-AGENT LLM SYSTEMS THROUGH STRUCTURED CONTEXTUAL MEMORY - OpenReview, https://openreview.net/pdf/3f7aaf02860909a16aed37a03ac26446b50c8161.pdf
19. Managing "Context Hell" with a Multi-Agent Stack (Claude Code, Gemini-CLI, Codex, Antigravity) – How do you consolidate? : r/ClaudeCode - Reddit, https://www.reddit.com/r/ClaudeCode/comments/1pjysfa/managing_context_hell_with_a_multiagent_stack/
20. Understanding Context Window for AI Performance & Use Cases - Qodo, https://www.qodo.ai/blog/context-windows/
21. Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems - arXiv, https://arxiv.org/html/2508.05687v1
22. Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems - Gradient Institute, https://www.gradientinstitute.org/assets/gradient_multiagent_report.pdf
23. Understanding and Mitigating Risks in AI Agents: A Threat Modelling Approach, https://edgeofthealgorithm.com/blog/20250523_aiagentthreatmodelling/
24. A unified framework for the architecture design of LLM-based autonomous agent, https://www.researchgate.net/figure/A-unified-framework-for-the-architecture-design-of-LLM-based-autonomous-agent_fig1_379217962
25. Do as We Do, Not as You Think: the Conformity of Large Language Models - arXiv, https://arxiv.org/html/2501.13381v1
26. Do as We Do, Not as You Think: the Conformity of Large Language Models - ChatPaper, https://chatpaper.com/paper/111485
27. [Literature Review] Conformity in Large Language Models - Moonlight, https://www.themoonlight.io/en/review/conformity-in-large-language-models
28. Report fixes multi-agent AI flaws - Public Spectrum, https://publicspectrum.co/report-fixes-multi-agent-ai-flaws/
29. (PDF) Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems, https://www.researchgate.net/publication/394427532_Risk_Analysis_Techniques_for_Governed_LLM-based_Multi-Agent_Systems
30. Why Do Multi-Agent LLM Systems Fail? - arXiv, https://arxiv.org/pdf/2503.13657
31. Why Do Multi-Agent LLM Systems Fail? - OpenReview, https://openreview.net/pdf?id=MqBzKkb8eK
32. Why Do Multi-Agent LLM Systems Fail? - arXiv, https://arxiv.org/html/2503.13657v1
33. WHY DO MULTI-AGENT LLM SYSTEMS FAIL? - OpenReview, https://openreview.net/pdf?id=wM521FqPvI
34. (PDF) Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents - ResearchGate, https://www.researchgate.net/publication/397521887_Towards_Outcome-Oriented_Task-Agnostic_Evaluation_of_AI_Agents
35. Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents - arXiv, https://arxiv.org/html/2511.08242v1
36. SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning - arXiv, https://arxiv.org/html/2503.11951v3
37. Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory - arXiv, https://arxiv.org/html/2508.08997v1
38. Multi-Agent LLM Systems: From Emergent Collaboration to Structured Collective Intelligence, https://www.preprints.org/manuscript/202511.1370/v1
